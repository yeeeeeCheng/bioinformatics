#####################################ATTENTION###################################
#                                                                               #
#     Please check and change the file name and path to the files that will     #
#     be processed later by this file.                                          #
#                                                                               #
#####################################ATTENTION###################################


#Change the working directory to the directory for RNA-Seq analysis pipeline.

cd /bioinformatics/RNA_Seq_Pipeline/

#Comment on the commands that don't want to be executed. Currently, this pipeline hasn't been executed without any comments, meaning executing from the top to the end in single run. However, it's important to check the log generated by each tool. Therefore, it's not recommended to run all the following commands in single run. Even though, some steps support batch query. Therefore, as long as there are enough space on the disk, pls feel free to append the commands for other files as many as u want. 

#Set the timer of the terminal to 0 so that the timer can record the time length for running the whole script.

SECONDS=0

#Step1. Fetch SRA Files from NCBI SRA Database Using SRA Toolkit. This step can be skipped for now as we our lab already has our own data can be analyzed. Still, if u want to try to play with it, pls refer to the official instruction of how to install and use it.

#First, we need to set up the PATH "each time" we are going to fetch SRA files from NCBI SRA Database using SRA Toolkit. This could be set up once and for all, yet it could involve the change of some base file in the computer, which should be handled carefully.

#export PATH=$PWD/apps/sratoolkit.ver-os-structure/bin:$PATH

#Verify whether SRA Toolkit can be executed successfully. Execute the following command. It should echoed the path of the file fastq-dump back.

#which fastq-dump

#Configure SRA Toolkit.

#vdb-config -i

#Now, a blue background should appear with some texts on it. Press tab and enter to move between different options and confirm choices, respectively. Make sure the box in the front of "Enable Remote Access" on the main page contained a X. Go to the "Cache" tab, and make sure the box in the front of "local file-caching" contained a X. Set the "Location of user-repository" to an empty directory, which will store the sra files fetched by the toolkit. At this step, we may have to type the route manually. At last, go to exit option and press enter to save and leave the configuration mode.

#After that, we can execute the following command in the next row to test whether SRA Toolkit can function normally. If the outputs are the first two result of the sequencing result with quality indices, it means the SRA Toolkit can function normally.

#fastq-dump --stdout -X 2 SRR390728

#Once the verifying procedure were all passed, SRA Toolkit can be used multiple times to fetch the SRA files we need without further setting. However, the above setting may be losted every time the terminal was shut down. As a result, it's important to verify whether this tool can function normally each time you're going to use it.

#Prefetch the SRA file. We can download SRA files without prefetching step, but the downloading process would be very long and cannot be rescued once the download process was disrupted. SRA files contained the information SRA Toolkit need to compile the whole fastq file, which require less storage than the fastq files. If the SRA files weren't deleted, the corresponding fastq files can be compiled multiple times. Also, if the prefetching step was interrupted, we can still resume the process by typing the same command again. With -p, we can see the progress bar so that we can take a grasp of how much of a file has already been fetched. Please change "_No" into the accession number of the sample of interest.

#prefetch -p SRR_No

#Once the prefetch files have already been fetched successfully, just execute the command in the following row to construct the fastq file we need. The detailed description of --split-files can be found in the documentation of SRA Toolkit on its pages in GitHub, as it can split the reads sequenced from the opposite ends into two files. Also, with the argument --outdir, we can specify the path we want to store the fastq files.

#fasterq-dump -p SRR_No --split-files --outdir results/Raw_reads/

#-------------------------------------------------------------------------------#

#Step2. Quality control using FastQC

#FastQC can be performed in two ways, in GUI or in CLI. Only the CLI can be incorporated into the pipeline within single run. Once the analysis was completed, FastQC will generate a HTML file report. The interpretation of the results can be found on the FastQC website. Also, FastQC and Cutadapt can examined the files stored as gz files. It's recommended to use gz files as inputs bc this way we can save a lot of space on the disk.

#To run FastQC in a CLI, put the main directory of FastQC to the the directory apps. 
#Move to the directory of FastQC.

#cd apps/FastQC

#Execute the following commands in the next row to make the user authorized to run the program.

#chmod u+x fastqc

#Next, we can perform FastQC on the fastq files we got that were stored in the "results/Raw_reads/" directory. Execute the following command to run FastQC. Please change the name of the .fa or .fa.gz file in the following command to specify the read files. More files can be added in the command to perform batch analysis as much as u want. The command "./fastqc" was used to call the program. The attribute -t specify how many threads can be allocated to FastQC.

#./fastqc ../../results/Raw_reads/Gz/MOCK-2_L1_1.fq.gz ../../results/Raw_reads/Gz/MOCK-2_L1_2.fq.gz -t 8

#Since FastQC generate the report the each fastq file, we can use MultiQC to gather all the report for all the fastq files in the Raw_reads folder.

#cd ../../results/Raw_reads/

#multiqc .

#-------------------------------------------------------------------------------#

#Step3. Trim adapter sequence and discard unwanted fragments with Cutadapt.

#Warning! One should not proceed without finishing the quality control! Also, since this library was sequenced in the paired-end mode, pls perform this step for the two files of the same sample simultaneously.

#Since Illumina has announced the adapter sequence publicly, we can manually input the adapter sequence following the -a and -A argument, respectively.
#Also, we can assign some strings with the path for the files for the input and output to shorten the main usage command.
#--pair-filter=any was used to discard the fragments when either one or both of the reads meet the criteria to be removed.
#Argument -j can be used to specify the threads we can allocate for the program to use.
#--nextseq-trim=30 can be used to detect the presence of poly-G and delete it even it got a Q score higher than 30.
#--poly-a was employed to detect poly-A and delete it no matter what.
#-m specify the minimun length of a fragment after modification, as fragments shorted than this length would be discarded.

#here we specify the untrimmed read pair files 

#read1=results/Raw_reads/Gz/MOCK-2_L1_1.fq.gz

#read2=results/Raw_reads/Gz/MOCK-2_L1_2.fq.gz

#and the trimmed read pair files

#output1=results/Clean_reads/Gz/MOCK-2_L1_1_clean.fq.gz

#output2=results/Clean_reads/Gz/MOCK-2_L1_2_clean.fq.gz

#cutadapt -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCA -A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT --nextseq-trim=30 --poly-a -m 50 --pair-filter=any -j 8 -o $output1 -p $output2 $read1 $read2

#After the trimming process was complete, we can perform FastQC again to confirm the status of the trimmed files meet our standard (or become the clean reads).

#cd apps/FastQC

#chmod u+x fastqc

#./fastqc ../../results/Clean_reads/Gz/MOCK-2_L1_1_clean.fq.gz ../../results/Clean_reads/Gz/MOCK-2_L1_2_clean.fq.gz -t 8

#cd ../../results/Clean_reads/Gz/

#multiqc .

#pls decompressed two gz files in the same directory at this point for further analysis.

#-------------------------------------------------------------------------------#

#Step4. Use HISAT2 to align the reads to the reference genome.

#export PATH=$PWD/apps/hisat2-2.2.1:$PATH

#To build the index for the target organism, use the following commands to generate the splice sites info and exons info with the gtf file. Sometimes the database won't provide the gtf file, but provide the gff file. We can convert the gff file with gffread program in the Cufflinks. Once the .ss and .exon files were generated, the "hisat2-build" command can be executed. -p specify the number of threads, --ss specify the path to the .ss file, and --exon specify the path to the .exon file. Then, just specify the path to the whole genome sequence fasta file and the name of the index. This step can take a very long time depending on the size of the genome. The better the processor is or the bigger the RAM is, the faster this step would be.

#gffread results/Indices/97103/97103_v2.5.gff3 -T -o results/Indices/97103/97103_v2.5.gtf

#hisat2_extract_splice_sites.py results/Indices/97103/97103_v2.5.gtf > results/Indices/97103/97103_v2.5.ss

#hisat2_extract_exons.py results/Indices/97103/97103_v2.5.gtf > results/Indices/97103/97103_v2.5.exon

#hisat2-build -p 8 --ss results/Indices/97103/97103_v2.5.ss --exon results/Indices/97103/97103_v2.5.exon results/Indices/97103/97103_genome_v2.5.fa results/Indices/97103/97103_genome_v2.5_idx

#Then, the following command is the main program of HISAT2. The "-q" means our input files come in the fastq format. The "-x" means the basename of the indices. Specify the strandness in the "--rna-strandness" attribute. samtools is employed in the pipeline to convert the sam files generated by HISAT2 to bam files simultaneously.

#If hisat2-build can't run and show "env: python no such file or directroy" message, it's probably because the default path of python in HISAT2 is different than that of python that was installed on this computer. Entering the following command to make a symlink, which is kind of like a temporary link of the PATH, so that the computer can temporarily add the python to the path we assigned, which is /usr/local/bin/python in this case.

#sudo ln -s /Library/Frameworks/Python.framework/Versions/3.13/bin/python3 /usr/local/bin/python

#hisat2 -q --rna-strandness FR -p 8 -x results/Indices/97103/97103_genome_v2.5_idx -1 results/Clean_reads/MOCK-2_L1_1_clean.fq -2 results/Clean_reads/MOCK-2_L1_2_clean.fq | samtools sort -o results/Aligned/Mock_2.bam

#-------------------------------------------------------------------------------#

#Step5. Quantify the reads with featureCounts.

#export PATH=$PWD/apps/subread-2.0.8-macOS-arm64/bin:$PATH

#cd results/Aligned/

#The usage of featureCounts require the annotation file comes in gtf format. It's recommended to move to the directory containing the bam files to make the output count matrix looks more tidy. -T specify the number of the threads, -p specify this library was sequenced in the paired-end mode, -a specify the path to the gtf file of the genome, and -o specify the path to store the output file. append the last part of the command based on how many sample u got.

#featureCounts -T 8 -p -a ../Indices/97103/97103_v2.5.gtf -o ../Quantification/97103/97103.txt Mock_2.bam

#cd ../Quantification/97103/

#multiqc can be used to check the status of the quantification.

#multiqc .

#Use the following pipeline to extract the info we need to construct the count matrix. The cat command can be used to call a certain file, while the perpendicular line represent a pipeline in this line that it can pass the result generated by cat command to the next command, which is the cut command, and so on. the cut command can be used to extract certain column of a tabular file. the tr command can transform the original delimiter into the one you want, here it change the tab delimiter into the comma one. finally, the > symbol means the final product should be saved as this file name and extension.

#cat 97103.txt | cut -f1,7-17 | tr '\t' ',' > 97103_count_matrix.csv
#cat 97103_sample_data.txt | tr '\t' ',' > 97103_sample_data.csv

#-------------------------------------------------------------------------------#

duration=$SECONDS
echo "$(($duration / 60)) minutes and $(($duration % 60)) seconds elapsed."